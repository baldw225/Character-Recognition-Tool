{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Recognition Tool\n",
    "\n",
    "Created by \n",
    "Joshua Baldwin | baldw225@msu.edu & Ralf Schmaelzle | schmaelz@msu.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install Packages - uncomment if needed\n",
    "#!pip install pliers\n",
    "#!pip install python-magic\n",
    "#!pip install face_recognition \n",
    "#!pip3 install --upgrade pandas\n",
    "#!pip install libmagic\n",
    "#!brew upgrade cmake\n",
    "#!homebrew, ffmpeg, dlib\n",
    "#!brew install  dlib\n",
    "#!brew install libmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, PIL, os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "\n",
    "from pliers.tests.utils import get_test_data_path\n",
    "from pliers.extractors import (FaceRecognitionFaceLocationsExtractor, FaceRecognitionFaceEncodingsExtractor, \n",
    "                               FaceRecognitionFaceLandmarksExtractor,SaliencyExtractor, BrightnessExtractor, merge_results)\n",
    "from pliers.filters import FrameSamplingFilter\n",
    "import face_recognition\n",
    "from pliers.graph import Graph\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide folder paths for input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name the Project Below\n",
    "project_name = 'Untitle'\n",
    "project_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Video\n",
    "video_reference = '../data/input_data/Untitle'\n",
    "video_reference\n",
    "print('This project will use ' + str(video_reference) + ' as the input video.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Reference Images \n",
    "reference_image_dir = '../data/input_data/reference_images/'\n",
    "character_images = [i for i in os.listdir(reference_image_dir) if i.endswith('.jpg')]\n",
    "n_character_images = len(character_images) \n",
    "print('The CRT will search for ' + str(n_character_images) + ' characters.')\n",
    "\n",
    "#What are the Names of the Characters (Alphabetical)?\n",
    "character_names = [x[:-4] for x in character_images ]\n",
    "n_characters = len(character_names)\n",
    "print('The CRT will identify the following characters: ')\n",
    "\n",
    "plt.figure(figsize=(18,4))\n",
    "for num, x in enumerate(character_images):\n",
    "    img = PIL.Image.open(os.path.join(reference_image_dir, x))\n",
    "    plt.subplot(1,n_characters ,num+1)\n",
    "    plt.title(x.split('.')[0])\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set frame rate and describe input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to value you prefer (values near 1 are more accurate, higher values will skip frames...)\n",
    "rate = 12\n",
    "print('We will work with a frame rate of ' + str(rate) + ' frames.')\n",
    "\n",
    "# change to values between 0 and 1 - values towards 0 are more strict, higher values more liberal,\n",
    "# usually values around 0.6 give best performance\n",
    "tolerance_threshold = 0.6\n",
    "\n",
    "model_to_use ='hog' #'hog' is faster, 'cnn' slower, but more accurate (if cnn, a gpu is advised)\n",
    "\n",
    "sampler = FrameSamplingFilter(every = rate)\n",
    "frames = sampler.transform(video_reference)\n",
    "n_frames = frames.n_frames\n",
    "n_frames_analyze = round(n_frames/24)\n",
    "print('The current extraction will work on ' + str(n_frames) + ' frames.')\n",
    "if (n_frames) < 100:\n",
    "    print('This should be pretty quick')\n",
    "else: \n",
    "    print('This may take a while')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up face-encodings of known images \n",
    "\n",
    "Describe what a face encoding means.... - possibly with some inline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_faces = []\n",
    "\n",
    "for character in character_images:\n",
    "    known_character_image =  face_recognition.load_image_file(reference_image_dir + character)\n",
    "    known_character_encoding = face_recognition.face_encodings(known_character_image)[0]\n",
    "    #print (character)\n",
    "    known_faces.append(known_character_encoding)\n",
    "    \n",
    "plt.figure(figsize = (14,8))\n",
    "plt.imshow(known_faces, cmap = 'seismic');\n",
    "plt.title('Face encoding vectors for the to-be-recognized characters')\n",
    "#plt.ylab\"el(character_images)\n",
    "#plt.colorbar();\n",
    "print(str(len(known_faces)) + ' character templates have been encoded and can now be searched.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.corrcoef(known_faces)\n",
    "mask =  np.triu(np.ones(A.shape), k=0)\n",
    "A = np.ma.array(A, mask=mask) # mask out the lower triangle\n",
    "\n",
    "plt.imshow(A, vmin=0.6, vmax=1, cmap='seismic');\n",
    "plt.colorbar();\n",
    "plt.figure(figsize=(18,4))\n",
    "\n",
    "\n",
    "for num, x in enumerate(character_images):\n",
    "    img = PIL.Image.open(os.path.join(reference_image_dir, x))\n",
    "    plt.subplot(1,n_characters ,num+1)\n",
    "    plt.title(x.split('.')[0] + str(num))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go through frames, detect faces, match them to templates, and create output\n",
    "\n",
    "This is the meat of the code...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unkonown_face_counter = 0\n",
    "res = []\n",
    "for curr_frame_number in np.arange(0, n_frames, 1):\n",
    "    sys.stdout.write(\" %d, \\r\" % (curr_frame_number) )\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # load the current frame as an image\n",
    "    curr_frame = frames.get_frame(curr_frame_number).video.get_frame(curr_frame_number).video.get_frame(curr_frame_number).data\n",
    "\n",
    "    # display for now\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(curr_frame)\n",
    "\n",
    "    # detect faces and plot them (for now) based on location info...\n",
    "    face_locations = face_recognition.face_locations(curr_frame, model= model_to_use) #'number_of_times_to_upsample=1', \"model='hog'\"\n",
    "    if len(face_locations)>0:\n",
    "        #print('I see a face!')\n",
    "        for curr_face in range(len(face_locations)):\n",
    "        \n",
    "            l1 = (face_locations[curr_face][2] - face_locations[curr_face][0])\n",
    "            l2 = (face_locations[curr_face][1] - face_locations[curr_face][3])\n",
    "            rect = patches.Rectangle((face_locations[curr_face][3], face_locations[curr_face][0]), l1,l2, edgecolor = 'r', facecolor='none')\n",
    "\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    \n",
    "    # for all detected faces, compute their encodings....\n",
    "    face_encodings = face_recognition.face_encodings(curr_frame, face_locations)\n",
    "\n",
    "    if len(face_encodings)>0 :\n",
    "        results = []\n",
    "        #n_recognized = 0\n",
    "        for curr_encoding in range(len(face_encodings)):\n",
    "            # compare the recognized & encoded faces with the known faces' encodings....\n",
    "            results.append(face_recognition.compare_faces( known_faces, \n",
    "                                                      face_encodings[curr_encoding], \n",
    "                                                      tolerance = tolerance_threshold))\n",
    "        results = np.sum(results, axis=0, dtype = 'bool') \n",
    "        #n_recognized = \n",
    "        \n",
    "        if (sum(results)>0):\n",
    "            \n",
    "            characters_recognized = [ character_names[i] for i in np.where(results)[0][:] ]\n",
    "            #print(characters_recognized)\n",
    "            print('I see the face of ' + str(characters_recognized) )\n",
    "            \n",
    "            #print('I see the face of ' + character_names[np.where(results)[0][0]])\n",
    "            #print(results)\n",
    "            #print()\n",
    "            print('--')\n",
    "            \n",
    "        #### This is where we do the surgery to add the unknown/feedback/human in the loop part\n",
    "        \n",
    "        # if more faces are detected than recognized, then we conclude that new faces are present\n",
    "        if (len(face_encodings) > sum(results)):\n",
    "            print('I see a face that I do not recognize, I save it to the reference_images folder.' )\n",
    "            \n",
    "            #cut out the new faces and put them in folder...\n",
    "            #for current_face_seen in range( len(face_encodings[0]) ):\n",
    "            \n",
    "            print(face_locations[0])\n",
    "            cc = curr_frame[    face_locations[0][0]:face_locations[0][2], \n",
    "                                face_locations[0][3]:face_locations[0][1],\n",
    "                                :]\n",
    "\n",
    "            plt.imshow(cc)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            from PIL import Image\n",
    "            im = Image.fromarray(cc)\n",
    "            unkonown_face_counter += 1\n",
    "            filename_uf = '../data/input_data/unknown_reference_images/unknown_face_' + str(unkonown_face_counter) + '.jpg'\n",
    "            \n",
    "\n",
    "            im.save(filename_uf)\n",
    "            \n",
    "            #face_recognition.compare_faces(known_faces, face_encodings[0])\n",
    "            \n",
    "        \n",
    "        \n",
    "        ####\n",
    "        \n",
    "        \n",
    "        #append the results...\n",
    "        res.append(results)\n",
    "    else:\n",
    "        res.append([False] * n_character_images)\n",
    "        \n",
    "res2 = np.asarray(res)\n",
    "np.asarray(res2[0])\n",
    "\n",
    "new_result = np.zeros((n_frames,n_characters))\n",
    "\n",
    "for this_line in range(n_frames):\n",
    "    new_result[this_line,:] = np.asarray(res2[this_line])\n",
    "\n",
    "df2 = pd.DataFrame( data    = new_result,\n",
    "                    columns = character_names );\n",
    "df2.to_csv('../data/output_data/' + project_name + '_face_recognition.csv')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(df2.values.T, cmap = 'seismic');\n",
    "#plt.axis('off');\n",
    "ax.set_yticks(np.arange(n_character_images));\n",
    "ax.set_yticklabels(np.array(character_images));\n",
    "ax.set_title(\"Character Recognitions\");\n",
    "ax.set_xlabel(\"Frames/Time\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
